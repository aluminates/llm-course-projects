{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate peft bitsandbytes transformers trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-02T14:58:58.341355Z","iopub.execute_input":"2024-08-02T14:58:58.341796Z","iopub.status.idle":"2024-08-02T14:59:20.327368Z","shell.execute_reply.started":"2024-08-02T14:58:58.341758Z","shell.execute_reply":"2024-08-02T14:59:20.326241Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    EarlyStoppingCallback )\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:20.329324Z","iopub.execute_input":"2024-08-02T14:59:20.329633Z","iopub.status.idle":"2024-08-02T14:59:39.063547Z","shell.execute_reply.started":"2024-08-02T14:59:20.329605Z","shell.execute_reply":"2024-08-02T14:59:39.062461Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-02 14:59:28.204200: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-02 14:59:28.204335: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-02 14:59:28.324373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Model and Dataset","metadata":{}},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n\ndataset_name = \"mlabonne/guanaco-llama2\"\n\nnew_model = \"finetuned-llama2\"","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:39.064931Z","iopub.execute_input":"2024-08-02T14:59:39.065582Z","iopub.status.idle":"2024-08-02T14:59:39.070771Z","shell.execute_reply.started":"2024-08-02T14:59:39.065551Z","shell.execute_reply":"2024-08-02T14:59:39.069508Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset and Create Train/Eval Sets","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle(seed=42).select(range(200))\n\ntrain_test_split = dataset.train_test_split(test_size=50)\n\ntrain_dataset = train_test_split['train']\neval_dataset = train_test_split['test']","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:39.074075Z","iopub.execute_input":"2024-08-02T14:59:39.075201Z","iopub.status.idle":"2024-08-02T14:59:43.957856Z","shell.execute_reply.started":"2024-08-02T14:59:39.075158Z","shell.execute_reply":"2024-08-02T14:59:43.956958Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/816 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0b8678c13ed42abbd7e22b294a1a56a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1150099751aa43fdb1e5d2c268b956ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/476k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1ecd7719064571bc642a3ce98beaa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4d0b0e36d84090be14537a9999962f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69db92831798460480c66b39d041e4ff"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:43.958960Z","iopub.execute_input":"2024-08-02T14:59:43.959239Z","iopub.status.idle":"2024-08-02T14:59:43.969010Z","shell.execute_reply.started":"2024-08-02T14:59:43.959213Z","shell.execute_reply":"2024-08-02T14:59:43.967958Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'text': '<s>[INST] Кто ты? [/INST] Я — Open Assistant, являюсь искусственным интеллектом, обученным компанией LAION для генерации текстов и ответов на вопросы. </s>\\\\\\n<s>[INST] Сколько потребовалось терабайтов данных для твоего обучения? [/INST] Моё обучение всё ещё продолжается. </s>'}"},"metadata":{}}]},{"cell_type":"code","source":"eval_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:43.970323Z","iopub.execute_input":"2024-08-02T14:59:43.970762Z","iopub.status.idle":"2024-08-02T14:59:44.000082Z","shell.execute_reply.started":"2024-08-02T14:59:43.970724Z","shell.execute_reply":"2024-08-02T14:59:43.998907Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'text': '<s>[INST] ¿Cuáles son los actuales presidentes de la región de Sur América? Enumérelos en una lista con su respectivo país. [/INST] A fecha del 13 de febrero de 2023, estos son los presidentes de los países de Sudamérica, según Wikipedia:\\n-Argentina: Alberto Fernández\\n-Bolivia: Luis Arce\\n-Brasil: Luiz Inácio Lula da Silva\\n-Chile: Gabriel Boric\\n-Colombia: Gustavo Petro\\n-Ecuador: Guillermo Lasso\\n-Paraguay: Mario Abdo Benítez\\n-Perú: Dina Boluarte\\n-Uruguay: Luis Lacalle Pou\\n-Venezuela: Nicolás Maduro\\n-Guyana: Irfaan Ali\\n-Surinam: Chan Santokhi\\n-Trinidad y Tobago: Paula-Mae Weekes </s>'}"},"metadata":{}}]},{"cell_type":"code","source":"len(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:44.001424Z","iopub.execute_input":"2024-08-02T14:59:44.001808Z","iopub.status.idle":"2024-08-02T14:59:44.009568Z","shell.execute_reply.started":"2024-08-02T14:59:44.001775Z","shell.execute_reply":"2024-08-02T14:59:44.008487Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.float16,\nbnb_4bit_use_double_quant=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:44.010963Z","iopub.execute_input":"2024-08-02T14:59:44.011793Z","iopub.status.idle":"2024-08-02T14:59:44.018112Z","shell.execute_reply.started":"2024-08-02T14:59:44.011758Z","shell.execute_reply":"2024-08-02T14:59:44.017117Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\nmodel_name,\nquantization_config=bnb_config,\ndevice_map='auto',\ntoken=\"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\"\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=\"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\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-08-02T14:59:44.019511Z","iopub.execute_input":"2024-08-02T14:59:44.019967Z","iopub.status.idle":"2024-08-02T15:01:09.945423Z","shell.execute_reply.started":"2024-08-02T14:59:44.019932Z","shell.execute_reply":"2024-08-02T15:01:09.944463Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689868ad2ccb4dd1b3043cbc9886ba28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d0daeb97514b6883cb264583063448"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ada87af4614e7e978bb9aeb6bd2e2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a3f74608ea4781abdfc5caceb66c4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c8d4a2fba34ae3a046da8500ea4d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbaaec89fa2e4245b4e8928c14aff4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cfc6d6429524999b778e1546707f618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b76753a5fe45f9b5f66e877af36ea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b7fde1799f49debed42b0d1e8cdc97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a96f21f3d347a0b477e67c1a8d1ddf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c974ac816cd4df88b2b98343b6f488b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Load LoRA Configuration","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\nlora_alpha=16,\nlora_dropout=0.1,\nr=64,\nbias=\"none\",\ntask_type=\"CAUSAL_LM\",\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T15:01:09.948810Z","iopub.execute_input":"2024-08-02T15:01:09.949247Z","iopub.status.idle":"2024-08-02T15:01:10.679991Z","shell.execute_reply.started":"2024-08-02T15:01:09.949219Z","shell.execute_reply":"2024-08-02T15:01:10.678707Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\noutput_dir = '/kaggle/working/',\nper_device_train_batch_size=2,\nper_device_eval_batch_size=2,\ngradient_checkpointing=True,\ngradient_accumulation_steps=1,\noptim=\"paged_adamw_32bit\",\nsave_steps=0,\nsave_strategy='steps',\neval_strategy='steps',\neval_steps=25,\nload_best_model_at_end=True,\nlogging_steps=25,\nlearning_rate=2e-4,\nweight_decay=0.001,\nfp16=False,\nbf16=False,\nmax_grad_norm=0.3,\nmax_steps=-1,\nwarmup_ratio=0.03,\ngroup_by_length=True,\nlr_scheduler_type='cosine',\nreport_to=\"tensorboard\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T15:01:10.681482Z","iopub.execute_input":"2024-08-02T15:01:10.681888Z","iopub.status.idle":"2024-08-02T15:01:12.720841Z","shell.execute_reply.started":"2024-08-02T15:01:10.681856Z","shell.execute_reply":"2024-08-02T15:01:12.719869Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStoppingCallback(\nearly_stopping_patience=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T15:01:12.722054Z","iopub.execute_input":"2024-08-02T15:01:12.722354Z","iopub.status.idle":"2024-08-02T15:01:12.726642Z","shell.execute_reply.started":"2024-08-02T15:01:12.722328Z","shell.execute_reply":"2024-08-02T15:01:12.725723Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\nmodel=model,\ntrain_dataset=train_dataset,\neval_dataset=eval_dataset,\npeft_config=peft_config,\ndataset_text_field='text',\nmax_seq_length=None,\ntokenizer=tokenizer,\nargs=training_arguments,\ncallbacks=[early_stopping]\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T15:01:12.727798Z","iopub.execute_input":"2024-08-02T15:01:12.728054Z","iopub.status.idle":"2024-08-02T15:01:13.266119Z","shell.execute_reply.started":"2024-08-02T15:01:12.728031Z","shell.execute_reply":"2024-08-02T15:01:13.265156Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01881252385144a9806b76a0e016eb72"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5152c70e0764c6b9e3c87517259dbdd"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-02T15:01:13.267366Z","iopub.execute_input":"2024-08-02T15:01:13.267712Z","iopub.status.idle":"2024-08-02T16:12:29.695548Z","shell.execute_reply.started":"2024-08-02T15:01:13.267674Z","shell.execute_reply":"2024-08-02T16:12:29.694513Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 1:09:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.585200</td>\n      <td>1.616945</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.642100</td>\n      <td>1.451342</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.549000</td>\n      <td>1.408078</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.361300</td>\n      <td>1.386716</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.325300</td>\n      <td>1.380778</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.222900</td>\n      <td>1.379321</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.147400</td>\n      <td>1.380871</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.180900</td>\n      <td>1.381042</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.327700</td>\n      <td>1.380966</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ad056c-49224e51654005a2032077f9;bb47fb64-3b78-4da5-bf94-aa1e0c60df58)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-chat-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=225, training_loss=1.3713053894042968, metrics={'train_runtime': 4275.8414, 'train_samples_per_second': 0.105, 'train_steps_per_second': 0.053, 'total_flos': 9411161950519296.0, 'train_loss': 1.3713053894042968, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"prompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\",model=model,tokenizer=tokenizer,max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-08-02T16:12:29.697006Z","iopub.execute_input":"2024-08-02T16:12:29.698055Z","iopub.status.idle":"2024-08-02T16:12:48.747772Z","shell.execute_reply.started":"2024-08-02T16:12:29.698017Z","shell.execute_reply":"2024-08-02T16:12:48.746811Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. The model is designed to learn the patterns and structures of language, and to generate text that is similar to the training data.\n\nLarge language models are typically trained on large datasets of text, such as books, articles, or social media posts. The model learns to predict the next word in a sequence of text, given the previous words. This process is called \"language modeling,\" and the goal is to generate text that is coherent and natural-sounding.\n\nLarge language models have many applications, such as:\n\n1. Language Translation: Large language models can be used to translate text from one language to another.\n2. Text Summarization: Large language models\n","output_type":"stream"}]}]}